{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12028eec-30d3-40c5-b449-786ca5edff6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551062832f25403684d5fa14c2ac987c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Hello', description='Input:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc8e52ff1f44126a970ffc496beb512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click Me', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d13c0ae9154b99b0c07af5032270bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "text = widgets.Text(value=\"Hello\", description=\"Input:\")\n",
    "button = widgets.Button(description=\"Click Me\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        print(f\"You entered: {text.value}\")\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(text)\n",
    "display(button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d4730f-dfbf-4d22-a5f7-fc0ecb5bae21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e904bf8aff6b4d6c90f100668dfed113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='flu', description='Keyword:', layout=Layout(width='500px'), placeholder='Enter a keyword (e.g., fl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04c34296de74405a31ae08814edfca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Search and Analyze', style=ButtonStyle(), tooltip='Click to search…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb55bc3707c4f0cb57630dfc739d0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"health_trends.log\", level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Load Reddit credentials\n",
    "CLIENT_ID = \"_r-auBrrOUMdCwudxBtZJw\"\n",
    "CLIENT_SECRET = \"ldQbRtpOltzZZHbJfDiuZGZq0s30bg\"\n",
    "USER_AGENT = \"windows:my-cool-app:v1.0 (by /u/Happy-Syllabub-6994)\"\n",
    "\n",
    "# Initialize geolocator for location inference\n",
    "geolocator = Nominatim(user_agent=\"health_trends_app\")\n",
    "locations = [\"New York\", \"California\", \"London\", \"Toronto\", \"Sydney\", \"Texas\", \"Florida\", \"Paris\", \"Tokyo\", \"Mumbai\"]\n",
    "\n",
    "# Create interactive widgets\n",
    "keyword_input = widgets.Text(\n",
    "    value=\"flu\",\n",
    "    placeholder=\"Enter a keyword (e.g., flu, fever)\",\n",
    "    description=\"Keyword:\",\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description=\"Search and Analyze\",\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Click to search Reddit and analyze posts\"\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define the analysis pipeline\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        keyword = keyword_input.value.strip()\n",
    "        if not keyword:\n",
    "            print(\"Please enter a keyword.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Collecting posts for keyword: {keyword}\")\n",
    "        logging.info(f\"Collecting posts for keyword: {keyword}\")\n",
    "\n",
    "        try:\n",
    "            # Authenticate with Reddit API\n",
    "            reddit = praw.Reddit(\n",
    "                client_id=CLIENT_ID,\n",
    "                client_secret=CLIENT_SECRET,\n",
    "                user_agent=USER_AGENT\n",
    "            )\n",
    "            reddit.read_only = True\n",
    "            user = reddit.user.me()\n",
    "            print(\"Authentication successful! User:\", user)\n",
    "            logging.info(\"Authentication successful\")\n",
    "\n",
    "            # Collect posts\n",
    "            posts = []\n",
    "            subreddit = reddit.subreddit(\"health+anxiety+mentalhealth+askdocs\")\n",
    "            for post in subreddit.search(keyword, limit=100):\n",
    "                posts.append({\n",
    "                    \"date\": post.created_utc,\n",
    "                    \"text\": post.title + \" \" + (post.selftext or \"\"),\n",
    "                    \"subreddit\": post.subreddit.display_name,\n",
    "                    \"username\": post.author.name if post.author else \"Unknown\",\n",
    "                    \"keyword\": keyword\n",
    "                })\n",
    "\n",
    "            if not posts:\n",
    "                print(\"No posts found for this keyword.\")\n",
    "                return\n",
    "\n",
    "            df = pd.DataFrame(posts)\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], unit=\"s\")\n",
    "            print(f\"Collected {len(posts)} posts\")\n",
    "            logging.info(f\"Collected {len(posts)} posts\")\n",
    "\n",
    "            # Step 1: Clean the data\n",
    "            def clean_text(text):\n",
    "                text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "                text = re.sub(r\"@\\w+|\\#\\w+\", \"\", text)\n",
    "                text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "                return text.strip()\n",
    "\n",
    "            df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)\n",
    "            df[\"subreddit\"] = df[\"subreddit\"].fillna(\"Unknown\")\n",
    "            df = df.drop_duplicates(subset=[\"text\"])\n",
    "            print(\"Data cleaned\")\n",
    "            logging.info(\"Data cleaned\")\n",
    "\n",
    "            # Step 2: Location inference\n",
    "            def extract_location(text):\n",
    "                for location in locations:\n",
    "                    if re.search(r'\\b' + re.escape(location) + r'\\b', text, re.IGNORECASE):\n",
    "                        try:\n",
    "                            geo = geolocator.geocode(location)\n",
    "                            if geo:\n",
    "                                return location, geo.latitude, geo.longitude\n",
    "                        except:\n",
    "                            continue\n",
    "                return None, None, None\n",
    "\n",
    "            df[\"location\"], df[\"latitude\"], df[\"longitude\"] = zip(*df[\"text\"].apply(extract_location))\n",
    "            print(\"Location data added\")\n",
    "            logging.info(\"Location data added\")\n",
    "\n",
    "            # Step 3: Sentiment analysis\n",
    "            def get_sentiment(text):\n",
    "                return TextBlob(text).sentiment.polarity\n",
    "\n",
    "            df[\"sentiment\"] = df[\"cleaned_text\"].apply(get_sentiment)\n",
    "            df[\"sentiment_category\"] = df[\"sentiment\"].apply(\n",
    "                lambda x: \"Positive\" if x > 0.1 else \"Negative\" if x < -0.1 else \"Neutral\"\n",
    "            )\n",
    "            print(\"Sentiment analysis completed\")\n",
    "            logging.info(\"Sentiment analysis completed\")\n",
    "\n",
    "            # Step 4: Topic modeling with scikit-learn LDA\n",
    "            texts = df[\"cleaned_text\"].dropna().tolist()\n",
    "            if not texts:\n",
    "                print(\"No valid texts for topic modeling.\")\n",
    "                df[\"topic\"] = None\n",
    "            else:\n",
    "                vectorizer = CountVectorizer(stop_words=\"english\", max_df=0.95, min_df=2)\n",
    "                doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "                lda_model = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "                lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "                df[\"topic\"] = [None] * len(df)\n",
    "                valid_indices = df[\"cleaned_text\"].dropna().index\n",
    "                df.loc[valid_indices, \"topic\"] = lda_output.argmax(axis=1)\n",
    "                print(\"Topic modeling completed\")\n",
    "                print(\"Identified Topics:\")\n",
    "                feature_names = vectorizer.get_feature_names_out()\n",
    "                for topic_idx, topic in enumerate(lda_model.components_):\n",
    "                    top_words = [feature_names[i] for i in topic.argsort()[:-6:-1]]\n",
    "                    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "                logging.info(\"Topic modeling completed\")\n",
    "\n",
    "            # Step 5: Time series analysis\n",
    "            daily_posts = df.groupby(df[\"date\"].dt.date).size().reset_index(name=\"post_count\")\n",
    "            daily_posts[\"date\"] = pd.to_datetime(daily_posts[\"date\"])\n",
    "            daily_posts.set_index(\"date\", inplace=True)\n",
    "            if len(daily_posts) > 7:\n",
    "                decomposition = seasonal_decompose(daily_posts[\"post_count\"], model=\"additive\", period=7)\n",
    "            else:\n",
    "                decomposition = None\n",
    "            print(\"Time series analysis completed\")\n",
    "            logging.info(\"Time series analysis completed\")\n",
    "\n",
    "            # Step 6: Visualizations\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            daily_posts.plot()\n",
    "            plt.title(f\"{keyword.capitalize()}-Related Posts Over Time\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Number of Posts\")\n",
    "            plt.show()\n",
    "\n",
    "            sentiment_counts = df[\"sentiment_category\"].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sentiment_counts.plot(kind=\"bar\")\n",
    "            plt.title(\"Sentiment Distribution\")\n",
    "            plt.xlabel(\"Sentiment\")\n",
    "            plt.ylabel(\"Number of Posts\")\n",
    "            plt.show()\n",
    "\n",
    "            topic_counts = df[\"topic\"].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            topic_counts.plot(kind=\"bar\")\n",
    "            plt.title(\"Topic Distribution\")\n",
    "            plt.xlabel(\"Topic\")\n",
    "            plt.ylabel(\"Number of Posts\")\n",
    "            plt.show()\n",
    "\n",
    "            location_df = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
    "            if not location_df.empty:\n",
    "                m = folium.Map(location=[0, 0], zoom_start=2)\n",
    "                marker_cluster = MarkerCluster().add_to(m)\n",
    "                for idx, row in location_df.iterrows():\n",
    "                    folium.Marker(\n",
    "                        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "                        popup=f\"{row['location']}: {row['text'][:100]}...\"\n",
    "                    ).add_to(marker_cluster)\n",
    "                display(m)\n",
    "            else:\n",
    "                print(\"No location data available for mapping.\")\n",
    "\n",
    "            if decomposition:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(411)\n",
    "                plt.plot(decomposition.observed, label=\"Observed\")\n",
    "                plt.legend(loc=\"best\")\n",
    "                plt.subplot(412)\n",
    "                plt.plot(decomposition.trend, label=\"Trend\")\n",
    "                plt.legend(loc=\"best\")\n",
    "                plt.subplot(413)\n",
    "                plt.plot(decomposition.seasonal, label=\"Seasonal\")\n",
    "                plt.legend(loc=\"best\")\n",
    "                plt.subplot(414)\n",
    "                plt.plot(decomposition.resid, label=\"Residual\")\n",
    "                plt.legend(loc=\"best\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"Not enough data for time series decomposition.\")\n",
    "\n",
    "            df.to_csv(\"analyzed_posts.csv\", index=False)\n",
    "            print(\"Analysis complete! Results saved to 'analyzed_posts.csv'\")\n",
    "            logging.info(\"Analysis complete\")\n",
    "\n",
    "        except praw.exceptions.RedditAPIException as e:\n",
    "            print(f\"Reddit API error: {e}\")\n",
    "            logging.error(f\"Reddit API error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            logging.error(f\"Error: {e}\")\n",
    "\n",
    "# Link the button to the function\n",
    "search_button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(keyword_input)\n",
    "display(search_button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20fa82-0625-4b57-bbb9-8d62bf68261d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (health)",
   "language": "python",
   "name": "health"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
